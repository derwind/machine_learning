{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sv2sULpSYOlx",
    "outputId": "ca251753-d111-4790-8bf7-254715263cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/157.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.5/157.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/139.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/14.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/14.1 MB\u001b[0m \u001b[31m165.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m251.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m251.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_4ES6K2nYWsP"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xbh2onbXY8wC"
   },
   "outputs": [],
   "source": [
    "ANIMALS = [\n",
    "  \"cat\",\n",
    "  \"caracal\",\n",
    "  \"capybara\",\n",
    "  \"canary\",\n",
    "  \"cavy\",\n",
    "  \"caiman\",\n",
    "  \"cacomistle\",\n",
    "  \"caribou\",\n",
    "  \"cassowary\",\n",
    "  \"caterpillar\",\n",
    "  \"dog\",\n",
    "  \"dalmatian\",\n",
    "  \"dachshund\",\n",
    "  \"doberman\",\n",
    "  \"duck\",\n",
    "  \"dingo\",\n",
    "  \"lion\",\n",
    "  \"tiger\",\n",
    "  \"leopard\",\n",
    "  \"cheetah\",\n",
    "  \"puma\",\n",
    "  \"jaguar\",\n",
    "  \"lynx\",\n",
    "  \"ocelot\",\n",
    "  \"serval\",\n",
    "  \"bobcat\",\n",
    "  \"cougar\",\n",
    "  \"panther\",\n",
    "  \"wolf\",\n",
    "  \"fox\",\n",
    "  \"jackal\",\n",
    "  \"coyote\",\n",
    "  \"hyena\",\n",
    "  \"bear\",\n",
    "  \"polar\",\n",
    "  \"grizzly\",\n",
    "  \"slothbear\",\n",
    "  \"panda\",\n",
    "  \"koala\",\n",
    "  \"kangaroo\",\n",
    "  \"wallaby\",\n",
    "  \"opossum\",\n",
    "  \"wombat\",\n",
    "  \"tasmanian\",\n",
    "  \"rabbit\",\n",
    "  \"hare\",\n",
    "  \"mouse\",\n",
    "  \"rat\",\n",
    "  \"gerbil\",\n",
    "  \"hamster\",\n",
    "  \"guinea\",\n",
    "  \"squirrel\",\n",
    "  \"chipmunk\",\n",
    "  \"beaver\",\n",
    "  \"porcupine\",\n",
    "  \"hedgehog\",\n",
    "  \"shrew\",\n",
    "  \"mole\",\n",
    "  \"bat\",\n",
    "  \"armadillo\",\n",
    "  \"antelope\",\n",
    "  \"gazelle\",\n",
    "  \"impala\",\n",
    "  \"gnu\",\n",
    "  \"eland\",\n",
    "  \"springbok\",\n",
    "  \"deer\",\n",
    "  \"moose\",\n",
    "  \"elk\",\n",
    "  \"reindeer\",\n",
    "  \"stag\",\n",
    "  \"doe\",\n",
    "  \"fawn\",\n",
    "  \"buffalo\",\n",
    "  \"bison\",\n",
    "  \"yak\",\n",
    "  \"zebu\",\n",
    "  \"cow\",\n",
    "  \"bull\",\n",
    "  \"ox\",\n",
    "  \"calf\",\n",
    "  \"sheep\",\n",
    "  \"lamb\",\n",
    "  \"ram\",\n",
    "  \"goat\",\n",
    "  \"kid\",\n",
    "  \"ibex\",\n",
    "  \"chamois\",\n",
    "  \"camel\",\n",
    "  \"dromedary\",\n",
    "  \"llama\",\n",
    "  \"alpaca\",\n",
    "  \"vicuna\",\n",
    "  \"horse\",\n",
    "  \"mare\",\n",
    "  \"stallion\",\n",
    "  \"colt\",\n",
    "  \"foal\",\n",
    "  \"donkey\",\n",
    "  \"mule\",\n",
    "  \"zebra\",\n",
    "  \"rhinoceros\",\n",
    "  \"hippopotamus\",\n",
    "  \"pig\",\n",
    "  \"boar\",\n",
    "  \"hog\",\n",
    "  \"swine\",\n",
    "  \"babirusa\",\n",
    "  \"tapir\",\n",
    "  \"elephant\",\n",
    "  \"mammoth\",\n",
    "  \"mastodon\",\n",
    "  \"dugong\",\n",
    "  \"manatee\",\n",
    "  \"whale\",\n",
    "  \"dolphin\",\n",
    "  \"porpoise\",\n",
    "  \"seal\",\n",
    "  \"seaotter\",\n",
    "  \"walrus\",\n",
    "  \"otter\",\n",
    "  \"weasel\",\n",
    "  \"ferret\",\n",
    "  \"marten\",\n",
    "  \"ermine\",\n",
    "  \"badger\",\n",
    "  \"skunk\",\n",
    "  \"wolverine\",\n",
    "  \"mongoose\",\n",
    "  \"meerkat\",\n",
    "  \"civet\",\n",
    "  \"genet\",\n",
    "  \"fossa\",\n",
    "  \"bearcat\",\n",
    "  \"platypus\",\n",
    "  \"echidna\",\n",
    "  \"pangolin\",\n",
    "  \"aardvark\",\n",
    "  \"aardwolf\",\n",
    "  \"okapi\",\n",
    "  \"giraffe\",\n",
    "  \"monkey\",\n",
    "  \"baboon\",\n",
    "  \"mandrill\",\n",
    "  \"macaque\",\n",
    "  \"langur\",\n",
    "  \"gibbon\",\n",
    "  \"gorilla\",\n",
    "  \"chimpanzee\",\n",
    "  \"bonobo\",\n",
    "  \"orangutan\",\n",
    "  \"lemur\",\n",
    "  \"tarsier\",\n",
    "  \"loris\",\n",
    "  \"ayeaye\",\n",
    "  \"sloth\",\n",
    "  \"anteater\",\n",
    "  \"tamandua\",\n",
    "  \"kitten\",\n",
    "  \"puppy\",\n",
    "  \"duckling\",\n",
    "  \"gosling\",\n",
    "  \"cygnet\",\n",
    "  \"eagle\",\n",
    "  \"hawk\",\n",
    "  \"falcon\",\n",
    "  \"osprey\",\n",
    "  \"vulture\",\n",
    "  \"buzzard\",\n",
    "  \"kite\",\n",
    "  \"owl\",\n",
    "  \"barnowl\",\n",
    "  \"tawnyowl\",\n",
    "  \"screechowl\",\n",
    "  \"snowyowl\",\n",
    "  \"parrot\",\n",
    "  \"macaw\",\n",
    "  \"cockatoo\",\n",
    "  \"budgerigar\",\n",
    "  \"lovebird\",\n",
    "  \"lorikeet\",\n",
    "  \"conure\",\n",
    "  \"parakeet\",\n",
    "  \"kingfisher\",\n",
    "  \"woodpecker\",\n",
    "  \"toucan\",\n",
    "  \"hornbill\",\n",
    "  \"cuckoo\",\n",
    "  \"cuckooshrike\",\n",
    "  \"nightjar\",\n",
    "  \"swift\",\n",
    "  \"hummingbird\",\n",
    "  \"swallow\",\n",
    "  \"martin\",\n",
    "  \"wren\",\n",
    "  \"warbler\",\n",
    "  \"thrush\",\n",
    "  \"blackbird\",\n",
    "  \"starling\",\n",
    "  \"mockingbird\",\n",
    "  \"finch\",\n",
    "  \"canary\",\n",
    "  \"sparrow\",\n",
    "  \"bunting\",\n",
    "  \"lark\",\n",
    "  \"pipit\",\n",
    "  \"wagtail\",\n",
    "  \"robin\",\n",
    "  \"chat\",\n",
    "  \"wheatear\",\n",
    "  \"dipper\",\n",
    "  \"nuthatch\",\n",
    "  \"treecreeper\",\n",
    "  \"tit\",\n",
    "  \"chickadee\",\n",
    "  \"jay\",\n",
    "  \"magpie\",\n",
    "  \"crow\",\n",
    "  \"raven\",\n",
    "  \"rook\",\n",
    "  \"jackdaw\",\n",
    "  \"chough\",\n",
    "  \"shrike\",\n",
    "  \"oriole\",\n",
    "  \"drongo\",\n",
    "  \"bulbul\",\n",
    "  \"mina\",\n",
    "  \"weaver\",\n",
    "  \"whydah\",\n",
    "  \"waxbill\",\n",
    "  \"munia\",\n",
    "  \"manakin\",\n",
    "  \"cotinga\",\n",
    "  \"antbird\",\n",
    "  \"ovenbird\",\n",
    "  \"woodcreeper\",\n",
    "  \"flycatcher\",\n",
    "  \"tyrant\",\n",
    "  \"pewee\",\n",
    "  \"kingbird\",\n",
    "  \"boatbill\",\n",
    "  \"motmot\",\n",
    "  \"tody\",\n",
    "  \"jacamar\",\n",
    "  \"puffbird\",\n",
    "  \"barbet\",\n",
    "  \"toucanet\",\n",
    "  \"ani\",\n",
    "  \"turaco\",\n",
    "  \"hoatzin\",\n",
    "  \"bustard\",\n",
    "  \"crane\",\n",
    "  \"heron\",\n",
    "  \"egret\",\n",
    "  \"bittern\",\n",
    "  \"stork\",\n",
    "  \"ibis\",\n",
    "  \"spoonbill\",\n",
    "  \"flamingo\",\n",
    "  \"swan\",\n",
    "  \"goose\",\n",
    "  \"teal\",\n",
    "  \"wigeon\",\n",
    "  \"shoveler\",\n",
    "  \"pintail\",\n",
    "  \"scaup\",\n",
    "  \"pochard\",\n",
    "  \"canvasback\",\n",
    "  \"redhead\",\n",
    "  \"goldeneye\",\n",
    "  \"merganser\",\n",
    "  \"eider\",\n",
    "  \"scoter\",\n",
    "  \"shelduck\",\n",
    "  \"woodduck\",\n",
    "  \"mandarin\",\n",
    "  \"mallard\",\n",
    "  \"gadwall\",\n",
    "  \"grebe\",\n",
    "  \"loon\",\n",
    "  \"penguin\",\n",
    "  \"albatross\",\n",
    "  \"petrel\",\n",
    "  \"shearwater\",\n",
    "  \"prion\",\n",
    "  \"stormpetrel\",\n",
    "  \"fulmar\",\n",
    "  \"gannet\",\n",
    "  \"booby\",\n",
    "  \"cormorant\",\n",
    "  \"shag\",\n",
    "  \"anhinga\",\n",
    "  \"frigatebird\",\n",
    "  \"tropicbird\",\n",
    "  \"pelican\",\n",
    "  \"darter\",\n",
    "  \"gull\",\n",
    "  \"tern\",\n",
    "  \"skimmer\",\n",
    "  \"auk\",\n",
    "  \"murre\",\n",
    "  \"puffin\",\n",
    "  \"guillemot\",\n",
    "  \"razorbill\",\n",
    "  \"dovekie\",\n",
    "  \"murrelet\",\n",
    "  \"kiwi\",\n",
    "  \"emu\",\n",
    "  \"rhea\",\n",
    "  \"ostrich\",\n",
    "  \"tinamou\",\n",
    "  \"rail\",\n",
    "  \"crake\",\n",
    "  \"gallinule\",\n",
    "  \"coot\",\n",
    "  \"limpkin\",\n",
    "  \"buttonquail\",\n",
    "  \"plover\",\n",
    "  \"lapwing\",\n",
    "  \"dotterel\",\n",
    "  \"killdeer\",\n",
    "  \"oystercatcher\",\n",
    "  \"avocet\",\n",
    "  \"stilt\",\n",
    "  \"phalarope\",\n",
    "  \"jacana\",\n",
    "  \"sandpiper\",\n",
    "  \"snipe\",\n",
    "  \"curlew\",\n",
    "  \"godwit\",\n",
    "  \"dowitcher\",\n",
    "  \"stint\",\n",
    "  \"ruff\",\n",
    "  \"turnstone\",\n",
    "  \"knot\",\n",
    "  \"pratincole\",\n",
    "  \"courser\",\n",
    "  \"skua\",\n",
    "  \"jaeger\",\n",
    "  \"eel\",\n",
    "  \"salmon\",\n",
    "  \"trout\",\n",
    "  \"carp\",\n",
    "  \"catfish\",\n",
    "  \"cobia\",\n",
    "  \"cod\",\n",
    "  \"coelacanth\",\n",
    "  \"flounder\",\n",
    "  \"goby\",\n",
    "  \"grouper\",\n",
    "  \"guppy\",\n",
    "  \"haddock\",\n",
    "  \"hake\",\n",
    "  \"halibut\",\n",
    "  \"koi\",\n",
    "  \"mackerel\",\n",
    "  \"minnow\",\n",
    "  \"perch\",\n",
    "  \"pike\",\n",
    "  \"pollock\",\n",
    "  \"sardine\",\n",
    "  \"shad\",\n",
    "  \"smelt\",\n",
    "  \"snapper\",\n",
    "  \"sole\",\n",
    "  \"sturgeon\",\n",
    "  \"tilapia\",\n",
    "  \"tuna\",\n",
    "  \"wahoo\",\n",
    "  \"zander\",\n",
    "  \"anchovy\",\n",
    "  \"barracuda\",\n",
    "  \"bass\",\n",
    "  \"blenny\",\n",
    "  \"bluegill\",\n",
    "  \"bonito\",\n",
    "  \"bream\",\n",
    "  \"butterfish\",\n",
    "  \"capelin\",\n",
    "  \"char\",\n",
    "  \"clownfish\",\n",
    "  \"drum\",\n",
    "  \"grunion\",\n",
    "  \"herring\",\n",
    "  \"killifish\",\n",
    "  \"lamprey\",\n",
    "  \"lionfish\",\n",
    "  \"loach\",\n",
    "  \"molly\",\n",
    "  \"mudskipper\",\n",
    "  \"needlefish\",\n",
    "  \"parrotfish\",\n",
    "  \"pompano\",\n",
    "  \"scad\",\n",
    "  \"sculpin\",\n",
    "  \"seahorse\",\n",
    "  \"shark\",\n",
    "  \"skate\",\n",
    "  \"sprat\",\n",
    "  \"sucker\",\n",
    "  \"sunfish\",\n",
    "  \"surgeonfish\",\n",
    "  \"tarpon\",\n",
    "  \"tetra\",\n",
    "  \"trevally\",\n",
    "  \"triggerfish\",\n",
    "  \"wrasse\",\n",
    "  \"tang\",\n",
    "  \"abalone\",\n",
    "  \"barnacle\",\n",
    "  \"clam\",\n",
    "  \"cockle\",\n",
    "  \"conch\",\n",
    "  \"crab\",\n",
    "  \"crawfish\",\n",
    "  \"krill\",\n",
    "  \"limpet\",\n",
    "  \"lobster\",\n",
    "  \"mussel\",\n",
    "  \"nautilus\",\n",
    "  \"oyster\",\n",
    "  \"periwinkle\",\n",
    "  \"prawn\",\n",
    "  \"scallop\",\n",
    "  \"shrimp\",\n",
    "  \"snail\",\n",
    "  \"squid\",\n",
    "  \"octopus\",\n",
    "  \"urchin\",\n",
    "  \"worm\",\n",
    "  \"beetle\",\n",
    "  \"butterfly\",\n",
    "  \"caterpillar\",\n",
    "  \"dragonfly\",\n",
    "  \"earwig\",\n",
    "  \"firefly\",\n",
    "  \"flea\",\n",
    "  \"grasshopper\",\n",
    "  \"ladybug\",\n",
    "  \"mantis\",\n",
    "  \"moth\",\n",
    "  \"termite\",\n",
    "  \"tick\",\n",
    "  \"wasp\",\n",
    "  \"weevil\",\n",
    "  \"aphid\",\n",
    "  \"ant\",\n",
    "  \"bee\",\n",
    "  \"bug\",\n",
    "  \"cricket\",\n",
    "  \"damselfly\",\n",
    "  \"fly\",\n",
    "  \"gnat\",\n",
    "  \"hornet\",\n",
    "  \"mayfly\",\n",
    "  \"mosquito\",\n",
    "  \"silverfish\",\n",
    "  \"spider\",\n",
    "  \"centipede\",\n",
    "  \"millipede\",\n",
    "  \"scorpion\",\n",
    "  \"copepod\",\n",
    "  \"isopod\",\n",
    "  \"amphipod\",\n",
    "  \"woodlouse\",\n",
    "  \"horseshoecrab\",\n",
    "  \"arachnid\",\n",
    "  \"mite\",\n",
    "  \"tarantula\",\n",
    "  \"fruitfly\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j3PCO5-6ZAMc"
   },
   "outputs": [],
   "source": [
    "FRUITS_VEGGIES = [\n",
    "  \"apple\",\n",
    "  \"apricot\",\n",
    "  \"avocado\",\n",
    "  \"artichoke\",\n",
    "  \"banana\",\n",
    "  \"bilberry\",\n",
    "  \"blackberry\",\n",
    "  \"blueberry\",\n",
    "  \"boysenberry\",\n",
    "  \"breadfruit\",\n",
    "  \"cantaloupe\",\n",
    "  \"casaba\",\n",
    "  \"carambola\",\n",
    "  \"cherimoya\",\n",
    "  \"cherry\",\n",
    "  \"cloudberry\",\n",
    "  \"coconut\",\n",
    "  \"cranberry\",\n",
    "  \"currant\",\n",
    "  \"date\",\n",
    "  \"elderberry\",\n",
    "  \"fig\",\n",
    "  \"gooseberry\",\n",
    "  \"grape\",\n",
    "  \"grapefruit\",\n",
    "  \"guava\",\n",
    "  \"honeydew\",\n",
    "  \"jackfruit\",\n",
    "  \"jambul\",\n",
    "  \"jujube\",\n",
    "  \"kiwi\",\n",
    "  \"kumquat\",\n",
    "  \"lemon\",\n",
    "  \"lime\",\n",
    "  \"loquat\",\n",
    "  \"lychee\",\n",
    "  \"mandarin\",\n",
    "  \"mango\",\n",
    "  \"mangosteen\",\n",
    "  \"melon\",\n",
    "  \"mulberry\",\n",
    "  \"nectarine\",\n",
    "  \"olive\",\n",
    "  \"orange\",\n",
    "  \"papaya\",\n",
    "  \"passionfruit\",\n",
    "  \"peach\",\n",
    "  \"pear\",\n",
    "  \"persimmon\",\n",
    "  \"pineapple\",\n",
    "  \"plum\",\n",
    "  \"pomegranate\",\n",
    "  \"pomelo\",\n",
    "  \"quince\",\n",
    "  \"raspberry\",\n",
    "  \"redcurrant\",\n",
    "  \"salak\",\n",
    "  \"satsuma\",\n",
    "  \"starfruit\",\n",
    "  \"strawberry\",\n",
    "  \"tamarillo\",\n",
    "  \"tamarind\",\n",
    "  \"tangelo\",\n",
    "  \"ugli\",\n",
    "  \"watermelon\",\n",
    "  \"yuzu\",\n",
    "  \"zucchini\",\n",
    "  \"carrot\",\n",
    "  \"cabbage\",\n",
    "  \"cauliflower\",\n",
    "  \"cassava\",\n",
    "  \"celery\",\n",
    "  \"chard\",\n",
    "  \"chicory\",\n",
    "  \"collard\",\n",
    "  \"corn\",\n",
    "  \"cress\",\n",
    "  \"cucumber\",\n",
    "  \"daikon\",\n",
    "  \"edamame\",\n",
    "  \"eggplant\",\n",
    "  \"endive\",\n",
    "  \"fennel\",\n",
    "  \"garlic\",\n",
    "  \"ginger\",\n",
    "  \"horseradish\",\n",
    "  \"jicama\",\n",
    "  \"kale\",\n",
    "  \"kohlrabi\",\n",
    "  \"leek\",\n",
    "  \"lettuce\",\n",
    "  \"okra\",\n",
    "  \"onion\",\n",
    "  \"parsnip\",\n",
    "  \"pea\",\n",
    "  \"pepper\",\n",
    "  \"potato\",\n",
    "  \"pumpkin\",\n",
    "  \"radish\",\n",
    "  \"rutabaga\",\n",
    "  \"shallot\",\n",
    "  \"spinach\",\n",
    "  \"squash\",\n",
    "  \"sweetcorn\",\n",
    "  \"sweetpotato\",\n",
    "  \"tomato\",\n",
    "  \"turnip\",\n",
    "  \"wasabi\",\n",
    "  \"yam\",\n",
    "  \"macadamia\",\n",
    "  \"pecan\",\n",
    "  \"cashew\",\n",
    "  \"hazelnut\",\n",
    "  \"walnut\",\n",
    "  \"almond\",\n",
    "  \"brazilnut\",\n",
    "  \"chestnut\",\n",
    "  \"pistachio\",\n",
    "  \"pine\",\n",
    "  \"acorn\",\n",
    "  \"watercress\",\n",
    "  \"caper\",\n",
    "  \"cardoon\",\n",
    "  \"canna\",\n",
    "  \"caraway\",\n",
    "  \"carob\",\n",
    "  \"camu\",\n",
    "  \"camote\",\n",
    "  \"canistel\",\n",
    "  \"canola\",\n",
    "  \"capers\",\n",
    "  \"carissa\",\n",
    "  \"catjang\",\n",
    "  \"cavendish\",\n",
    "  \"cayenne\",\n",
    "  \"celeriac\",\n",
    "  \"chayote\",\n",
    "  \"cilantro\",\n",
    "  \"clementine\",\n",
    "  \"cornsalad\",\n",
    "  \"courgette\",\n",
    "  \"currant\",\n",
    "  \"cushaw\",\n",
    "  \"dandelion\",\n",
    "  \"dill\",\n",
    "  \"durian\",\n",
    "  \"endive\",\n",
    "  \"escarole\",\n",
    "  \"fiddlehead\",\n",
    "  \"frisee\",\n",
    "  \"gourd\",\n",
    "  \"jostaberry\",\n",
    "  \"kohlrabi\",\n",
    "  \"lablab\",\n",
    "  \"luffa\",\n",
    "  \"malanga\",\n",
    "  \"mangetout\",\n",
    "  \"mungbean\",\n",
    "  \"navybean\",\n",
    "  \"nopale\",\n",
    "  \"onionchive\",\n",
    "  \"parsley\",\n",
    "  \"parsnip\",\n",
    "  \"pattypan\",\n",
    "  \"peasnap\",\n",
    "  \"persimmon\",\n",
    "  \"pigeonpea\",\n",
    "  \"plantain\",\n",
    "  \"pluot\",\n",
    "  \"pomegranate\",\n",
    "  \"prune\",\n",
    "  \"pumpkin\",\n",
    "  \"radicchio\",\n",
    "  \"rambutan\",\n",
    "  \"rapini\",\n",
    "  \"rocket\",\n",
    "  \"rutabaga\",\n",
    "  \"salsify\",\n",
    "  \"sapote\",\n",
    "  \"scallion\",\n",
    "  \"shallot\",\n",
    "  \"snowpea\",\n",
    "  \"sorrel\",\n",
    "  \"soybean\",\n",
    "  \"spelt\",\n",
    "  \"squash\",\n",
    "  \"tamarind\",\n",
    "  \"tangelo\",\n",
    "  \"tatsoi\",\n",
    "  \"tomatillo\",\n",
    "  \"tuber\",\n",
    "  \"turnip\",\n",
    "  \"waterchestnut\",\n",
    "  \"watermelon\",\n",
    "  \"waxgourd\",\n",
    "  \"yambean\",\n",
    "  \"yautia\",\n",
    "  \"yuca\",\n",
    "  \"ziziphus\",\n",
    "  \"zucchini\",\n",
    "  \"acerola\",\n",
    "  \"ackee\",\n",
    "  \"ambarella\",\n",
    "  \"arugula\",\n",
    "  \"asparagus\",\n",
    "  \"azuki\",\n",
    "  \"bamboo\",\n",
    "  \"basil\",\n",
    "  \"bean\",\n",
    "  \"beet\",\n",
    "  \"bellpepper\",\n",
    "  \"betel\",\n",
    "  \"bokchoy\",\n",
    "  \"broccoli\",\n",
    "  \"broccolini\",\n",
    "  \"brusselsprout\",\n",
    "  \"burdock\",\n",
    "  \"butternut\",\n",
    "  \"calabash\",\n",
    "  \"calamansi\",\n",
    "  \"canarymelon\",\n",
    "  \"cantaloupe\",\n",
    "  \"capuli\",\n",
    "  \"carambola\",\n",
    "  \"carrot\",\n",
    "  \"cassava\",\n",
    "  \"cauliflower\",\n",
    "  \"celery\",\n",
    "  \"chamomile\",\n",
    "  \"cherry\",\n",
    "  \"chickpea\",\n",
    "  \"chicory\",\n",
    "  \"chives\",\n",
    "  \"cilantro\",\n",
    "  \"citrus\",\n",
    "  \"collards\",\n",
    "  \"coriander\",\n",
    "  \"courgette\",\n",
    "  \"cranberry\",\n",
    "  \"cress\",\n",
    "  \"cucumber\",\n",
    "  \"currant\",\n",
    "  \"daikon\",\n",
    "  \"dandelion\",\n",
    "  \"dates\",\n",
    "  \"dragonfruit\",\n",
    "  \"durian\",\n",
    "  \"eggplant\",\n",
    "  \"elderberry\",\n",
    "  \"endive\",\n",
    "  \"fennel\",\n",
    "  \"feijoa\",\n",
    "  \"fig\",\n",
    "  \"fiddlehead\",\n",
    "  \"garbanzo\",\n",
    "  \"garlic\",\n",
    "  \"ginger\",\n",
    "  \"gooseberry\",\n",
    "  \"grape\",\n",
    "  \"grapefruit\",\n",
    "  \"guava\",\n",
    "  \"habanero\",\n",
    "  \"honeydew\",\n",
    "  \"horseradish\",\n",
    "  \"iceberg\",\n",
    "  \"jackfruit\",\n",
    "  \"jalapeno\",\n",
    "  \"jicama\",\n",
    "  \"jostaberry\",\n",
    "  \"jujube\",\n",
    "  \"kabocha\",\n",
    "  \"kale\",\n",
    "  \"kiwi\",\n",
    "  \"kohlrabi\",\n",
    "  \"kumquat\",\n",
    "  \"leek\",\n",
    "  \"lemon\",\n",
    "  \"lentil\",\n",
    "  \"lettuce\",\n",
    "  \"licorice\",\n",
    "  \"lime\",\n",
    "  \"lingonberry\",\n",
    "  \"loquat\",\n",
    "  \"luffa\",\n",
    "  \"lychee\",\n",
    "  \"maca\",\n",
    "  \"mandarin\",\n",
    "  \"mango\",\n",
    "  \"mangosteen\",\n",
    "  \"marrow\",\n",
    "  \"melon\",\n",
    "  \"mungbean\",\n",
    "  \"mustard\",\n",
    "  \"nectarine\",\n",
    "  \"okra\",\n",
    "  \"olive\",\n",
    "  \"onion\",\n",
    "  \"orange\",\n",
    "  \"oregano\",\n",
    "  \"papaya\",\n",
    "  \"parsley\",\n",
    "  \"parsnip\",\n",
    "  \"passionfruit\",\n",
    "  \"pea\",\n",
    "  \"peach\",\n",
    "  \"peanut\",\n",
    "  \"pear\",\n",
    "  \"pecan\",\n",
    "  \"pepper\",\n",
    "  \"persimmon\",\n",
    "  \"pineapple\",\n",
    "  \"pistachio\",\n",
    "  \"plum\",\n",
    "  \"pomegranate\",\n",
    "  \"pomelo\",\n",
    "  \"potato\",\n",
    "  \"pumpkin\",\n",
    "  \"quince\",\n",
    "  \"radish\",\n",
    "  \"rambutan\",\n",
    "  \"rapini\",\n",
    "  \"raspberry\",\n",
    "  \"redcurrant\",\n",
    "  \"rhubarb\",\n",
    "  \"rocket\",\n",
    "  \"rutabaga\",\n",
    "  \"salsify\",\n",
    "  \"sapote\",\n",
    "  \"scallion\",\n",
    "  \"shallot\",\n",
    "  \"snappea\",\n",
    "  \"sorrel\",\n",
    "  \"soybean\",\n",
    "  \"spinach\",\n",
    "  \"spelt\",\n",
    "  \"squash\",\n",
    "  \"starfruit\",\n",
    "  \"strawberry\",\n",
    "  \"sweetcorn\",\n",
    "  \"sweetpotato\",\n",
    "  \"tamarillo\",\n",
    "  \"tamarind\",\n",
    "  \"tangelo\",\n",
    "  \"tatsoi\",\n",
    "  \"tomatillo\",\n",
    "  \"tomato\",\n",
    "  \"turnip\",\n",
    "  \"ugli\",\n",
    "  \"watercress\",\n",
    "  \"watermelon\",\n",
    "  \"waxgourd\",\n",
    "  \"yam\",\n",
    "  \"yuca\",\n",
    "  \"ziziphus\",\n",
    "  \"zucchini\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dGgot3tYZBmz"
   },
   "outputs": [],
   "source": [
    "NAMES = ANIMALS + FRUITS_VEGGIES\n",
    "\n",
    "# ----- トークン化 -----\n",
    "ALL_CHARS = sorted(set(\"\".join(NAMES)))\n",
    "SPECIAL_TOKENS = [\"<PAD>\", \"<BOS>\", \"<EOS>\"]\n",
    "ALL_TOKENS = SPECIAL_TOKENS + ALL_CHARS\n",
    "VOCAB_SIZE = len(ALL_TOKENS)\n",
    "CHAR2IDX = {ch: i for i, ch in enumerate(ALL_TOKENS)}\n",
    "IDX2CHAR = {i: ch for ch, i in CHAR2IDX.items()}\n",
    "PAD_IDX = CHAR2IDX[\"<PAD>\"]\n",
    "BOS_IDX = CHAR2IDX[\"<BOS>\"]\n",
    "EOS_IDX = CHAR2IDX[\"<EOS>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uosT4kASZFtR"
   },
   "outputs": [],
   "source": [
    "def encode_word(word, max_len):\n",
    "    tokens = [BOS_IDX] + [CHAR2IDX[c] for c in word] + [EOS_IDX]\n",
    "    tokens += [PAD_IDX] * (max_len - len(tokens))\n",
    "    return tokens\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    chars = []\n",
    "    for idx in tokens:\n",
    "        if idx == EOS_IDX:\n",
    "            break\n",
    "        if idx >= len(IDX2CHAR):\n",
    "            continue\n",
    "        ch = IDX2CHAR[idx]\n",
    "        if ch not in SPECIAL_TOKENS:\n",
    "            chars.append(ch)\n",
    "    return \"\".join(chars)\n",
    "\n",
    "# ----- データセットクラス -----\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, words, max_len):\n",
    "        self.max_len = max_len\n",
    "        self.data = []\n",
    "        for w in words:\n",
    "            tokens = encode_word(w, max_len)\n",
    "            self.data.append(tokens)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx]\n",
    "        x = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lEbcZnpHZLVQ"
   },
   "outputs": [],
   "source": [
    "# ----- 位置エンコーディング -----\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model > 1:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1)]\n",
    "\n",
    "# ----- MiniGPT2本体（Pre-LN） -----\n",
    "class MiniGPT2(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=32, max_len=16):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.q_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.attn_out = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model, bias=False)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.max_len = max_len\n",
    "        self.attn_weights = None\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        emb = self.embed(x)\n",
    "        emb = self.pos_enc(emb)\n",
    "        # Pre-LN構造：LN→Attention→Add\n",
    "        attn_in = self.ln1(emb)\n",
    "        Q = self.q_linear(attn_in)\n",
    "        K = self.k_linear(attn_in)\n",
    "        V = self.v_linear(attn_in)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_model)\n",
    "        mask = torch.triu(torch.ones(scores.size(-2), scores.size(-1)), diagonal=1).bool().to(x.device)\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn_out = torch.matmul(attn, V)\n",
    "        attn_out = self.attn_out(attn_out)\n",
    "        x1 = emb + attn_out\n",
    "        # Pre-LN→FFN→Add\n",
    "        ffn_in = self.ln2(x1)\n",
    "        x2 = x1 + self.ffn(ffn_in)\n",
    "        logits = torch.matmul(x2, self.embed.weight.t())\n",
    "        if return_attn:\n",
    "            self.attn_weights = attn.detach().cpu().numpy()\n",
    "            return logits, attn\n",
    "        return logits\n",
    "\n",
    "    def generate(self, start_tokens, eos_idx, pad_idx, max_gen=None, temperature=1.0):\n",
    "        self.eval()\n",
    "        max_gen = max_gen or self.max_len\n",
    "        tokens = start_tokens.tolist()\n",
    "        for _ in range(max_gen - len(tokens)):\n",
    "            inp = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(next(self.parameters()).device)\n",
    "            logits = self.forward(inp)\n",
    "            next_token_logits = logits[0, len(tokens)-1] / temperature\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            if next_token == eos_idx:\n",
    "                break\n",
    "            tokens.append(next_token)\n",
    "        while len(tokens) < self.max_len:\n",
    "            tokens.append(pad_idx)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5HuRSk64ZZWT"
   },
   "outputs": [],
   "source": [
    "# ----- 訓練・評価 -----\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Caa7rxESZf3a",
    "outputId": "75103199-a2c8-49bf-ae02-a40ba172b1b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5: train loss=3.2279, test loss=3.1276\n",
      "Epoch 10: train loss=2.5994, test loss=2.6855\n",
      "Epoch 15: train loss=2.4942, test loss=2.6094\n",
      "Epoch 20: train loss=2.4358, test loss=2.5974\n",
      "Epoch 25: train loss=2.4010, test loss=2.5769\n",
      "Epoch 30: train loss=2.3862, test loss=2.5467\n",
      "Epoch 35: train loss=2.3552, test loss=2.5494\n",
      "Epoch 40: train loss=2.3404, test loss=2.5146\n",
      "Epoch 45: train loss=2.3369, test loss=2.5244\n",
      "Epoch 50: train loss=2.3236, test loss=2.4903\n",
      "Epoch 55: train loss=2.2990, test loss=2.5180\n",
      "Epoch 60: train loss=2.2725, test loss=2.4973\n",
      "Epoch 65: train loss=2.2702, test loss=2.4950\n",
      "Epoch 70: train loss=2.2713, test loss=2.5089\n",
      "Epoch 75: train loss=2.2495, test loss=2.4875\n",
      "Epoch 80: train loss=2.2398, test loss=2.4760\n",
      "Epoch 85: train loss=2.2418, test loss=2.5207\n",
      "Epoch 90: train loss=2.2428, test loss=2.4992\n",
      "Epoch 95: train loss=2.2248, test loss=2.4787\n",
      "Epoch 100: train loss=2.2263, test loss=2.4894\n",
      "CPU times: user 18.7 s, sys: 710 ms, total: 19.4 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ----- 設定 -----\n",
    "max_word_len = max(len(w) for w in NAMES) + 2 # BOS, EOS\n",
    "batch_size = 16\n",
    "d_model = 32\n",
    "n_epochs = 100\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ----- データ分割 -----\n",
    "random.seed(42)\n",
    "random.shuffle(NAMES)\n",
    "split = int(len(NAMES) * 0.8)\n",
    "train_words = NAMES[:split]\n",
    "test_words = NAMES[split:]\n",
    "\n",
    "train_ds = NameDataset(train_words, max_word_len)\n",
    "test_ds = NameDataset(test_words, max_word_len)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ----- モデル -----\n",
    "model = MiniGPT2(VOCAB_SIZE, d_model, max_word_len).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# ----- 学習ループ -----\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss = evaluate(model, test_loader, criterion, device)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:2d}: train loss={train_loss:.4f}, test loss={test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5NiRc2lKZ0gg",
    "outputId": "e9a33625-18ac-4a6e-9378-2ffde21a44eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成例: categow\n"
     ]
    }
   ],
   "source": [
    "# ----- 生成例 -----\n",
    "def sample_generate(prompt, model, max_word_len, temperature=1.0):\n",
    "    start_tokens = [BOS_IDX] + [CHAR2IDX[c] for c in prompt]\n",
    "    start_tokens = torch.tensor(start_tokens, dtype=torch.long)\n",
    "    out_tokens = model.generate(start_tokens, eos_idx=EOS_IDX, pad_idx=PAD_IDX, max_gen=max_word_len, temperature=temperature)\n",
    "    return decode_tokens(out_tokens[1:])  # BOSを除く\n",
    "\n",
    "# ↓ 例： \"ca\" で始まる動物名っぽいものを生成\n",
    "print(\"生成例:\", sample_generate(\"ca\", model, max_word_len, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQKH8YtQaAFj",
    "outputId": "85c0f01b-5c03-4e89-bb89-fc3d6f62438a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成例: dock\n"
     ]
    }
   ],
   "source": [
    "print(\"生成例:\", sample_generate(\"do\", model, max_word_len, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_B0JrtITaEsU",
    "outputId": "5941315a-4f58-48aa-f5d5-defd5978b092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成例: appot\n"
     ]
    }
   ],
   "source": [
    "print(\"生成例:\", sample_generate(\"app\", model, max_word_len, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1jDukAqaHdg",
    "outputId": "1d73ef7b-6f17-4ba3-86e9-c33188437a12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成例: shanelon\n"
     ]
    }
   ],
   "source": [
    "print(\"生成例:\", sample_generate(\"sh\", model, max_word_len, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDOPbeolacBV",
    "outputId": "6d6e215d-0bb8-480d-80b9-64b8f90bbc5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成例: cabepa\n"
     ]
    }
   ],
   "source": [
    "print(\"生成例:\", sample_generate(\"ca\", model, max_word_len, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7D_H_16adxe",
    "outputId": "623359c5-9af8-4ce5-c80f-8ec078d71fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成例: cander\n"
     ]
    }
   ],
   "source": [
    "print(\"生成例:\", sample_generate(\"ca\", model, max_word_len, temperature=0.8))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
