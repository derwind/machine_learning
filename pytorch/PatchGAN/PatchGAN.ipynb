{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51bebf-f04e-46e6-919d-125bf8dac43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d17b7b-afc4-468b-b121-cc709fd0dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((64, 64), transforms.InterpolationMode.BICUBIC),\n",
    "                                transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d7e14-4e00-4c41-894e-b05322809cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dataset with enough size\n",
    "train_data = torchvision.datasets.STL10(\"~/.pytorch/STL10_data\", split=\"train\", download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf0775-e172-4f8a-9e1b-bbab7ab30a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, n):\n",
    "    images = images.detach().numpy()\n",
    "    for i in range(n):\n",
    "        # bcwh->whc\n",
    "        imgs = np.transpose(images[i], [1,2,0])\n",
    "        plt.imshow(imgs)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56bc5a6-dfc2-4fb4-91e8-16892efe409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample images\n",
    "show_images(next(iter(train_loader))[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa5c28c-8a69-4eeb-90fd-fa1f8a87cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make trivial conv2d but downsample images to half size\n",
    "conv1 = nn.Conv2d(3, 3, 4, stride=2, padding=1)\n",
    "kernel = torch.stack([\n",
    "    torch.stack([torch.ones(4, 4)/16, torch.zeros(4, 4), torch.zeros(4, 4)]),\n",
    "    torch.stack([torch.zeros(4, 4), torch.ones(4, 4)/16, torch.zeros(4, 4)]),\n",
    "    torch.stack([torch.zeros(4, 4), torch.zeros(4, 4), torch.ones(4, 4)/16])\n",
    "])\n",
    "assert kernel.shape == conv1.weight.shape\n",
    "bias = torch.zeros_like(conv1.bias)\n",
    "conv1.weight = Parameter(kernel)\n",
    "conv1.bias = Parameter(bias)\n",
    "\n",
    "conv2 = nn.Conv2d(3, 3, 4, stride=2, padding=1)\n",
    "conv2.weight = Parameter(kernel)\n",
    "conv2.bias = Parameter(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ad750-0d6f-46f9-bc6d-72ce73acfca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show output features\n",
    "imgs, labels = next(iter(train_loader))\n",
    "output1 = conv1(imgs)\n",
    "output2 = conv2(output1)\n",
    "print(imgs.shape, output1.shape, output2.shape)\n",
    "show_images(imgs, 3)\n",
    "show_images(output1, 3)\n",
    "show_images(output2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dabae9-f518-47d1-82fb-9f8714dc2fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/cyclegan/models.py\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        channels, height, width = input_shape\n",
    "\n",
    "        # Calculate output shape of image discriminator (PatchGAN)\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff147b6-b7ad-4ee4-96fa-a9c96e3c3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator((3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1934e6-ef7d-4e32-bda2-ea2daf974967",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(train_loader))\n",
    "output = disc(imgs)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
